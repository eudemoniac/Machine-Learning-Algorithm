# Boosting

## Adaboost (adaptive boosting)

### 思想

我们可以启发式地得到一些简单、效果欠佳的算法。而如果我们可以站在前面算法得到的分类器的基础上，弥补他们的错误，循环的训练出一些弱分类器，最后将他们和在一起，变成一个强学习算法。

这种boosting包含两个步骤：

	1. 弥补错误：将上一个分类器分错的点的权值（在计算loss中）变大
	1. 联合：将每个简单分类器的正确率作为权值，将他们线性叠加为强学习分类器

### 算法

#### notation

- M种分类器：m：1～M
- D：第m个分类器中第i个样本{wmi}的分布
- 第m个分类器的误差：em
- 分类器的权重alpham = 1/2 * log((1-em)/em)

#### 算法

1. 用算法Am，在Dm上训练出最优分类器Gm

2. 计算em，am

3. 更新Dm+1

4. 循环

   最后，将分类器Gm按照am线性组合，得到最终分类器

### 误差分析

#### 训练误差界

最终训练误差 小于 Zm的乘积

在选取Gm时应使得Zm最小，从而使loss下降最快

#### 二分类时的训练误差界

以指数下降

## 算法解释

- 加法模型（最后模型是线性叠加）
- 二分类模型
- 向前分布算法

### 向前逐步算法

将模型依次加入总模型中

## Boosting Tree （one of the most efficient method)

二分类回归树

此时用「回归fn-1(x)的残差」来代替「提升错误样本权重」

## XGboost

用损失函数的负梯度（作为残差的近似值）作为作为拟合的准则